1. Two Sided Send & Recv with Sleep (P2P)

    0. Goal
        1. Show the hybrid model of OpenMPs with MPIs has a much worse performance (in terms of normalized throughput) in comparison
            with pure MPIs.
        2. Show what the throughput performances of hybrid model are when the level of parallization (number of OpenMP threads) goes up.
        3. Show what the throughput performance of hybrid model is when interleaving between threads (not all threads send and recv messages simultaneously)
            is enabled.
        4. Show what the throughput performance of hybrid model is when, given the level of parallization, the varying length of sleeping time
            influences the throughput
        (See section 3 for more detail)

    1. Overview
        1. Create 2 * n MPIs / OpenMP threads using MPI Library to send and recv messages
        2. Each member busy sends and busy receives messages to & from its counterpart (id: 2x <-> id: 2x+1)
        3. Use USLEEP to simulate computation
        4. Do MPI communications within a BATCH to get more accurate results due to the float accuracy
            (currently BATCH_SIZE = 5 rounds of nonstop recv & send).
            Not really important as we are not interested in latency.
        5. We are interested in the throughput. The higher the throughput, the higher parallization between process is.
            Not really interested in the message latency due to the mismatch between MPIs / threads
            as each process / thread sleeps for a random amount of time.
        6. Use two sets of send & recv operations
            MPI_Send & MPI_Recv
            MPI_Isend & MPI_Irecv


    2. Benchmark Code Flow
          get start_timestamp
          for each iteration:
              for each iteration within the batch:
                  Send message to the counterpart
                  Recv message from the counterpart
              end;
              if using async Isend & Irecv
                  wait for the Isend & Irecv to finish
              end;
              USLEEP for a period of time (SLEEP_BASE + RANDOM(SLEEP_FLUCTUATION)) us to simulate some computation is going on
          end;
          get end_timestamp;
          Calculate actual throughput = (NUM_MESSAGES * MESSAGE_SIZE) / time_elapsed;
          Calculate normalized throughput = (NUM_MESSAGES * MESSAGE_SIZE) / (time_elapsed - time_of_sleep);

    3. Benchmark Detail
        1. Compare throughput between applications using multiple MPIs and multiple OpenMP threads.
        2. Randomly pick a process / thread to dump the output.
        3. Normalized throughput is the main metrics we will be using as it removes the sleeping time from the total time elapsed.
            and should be relatively independent from the amount of sleeping time.
        4. Three comparison methods
            1.  Given same level of parallization (10 MPIs <-> 10 MPIs vs 10 OpenMPs <-> 10 OpenMPs)
                  Tune the sleeping time to see how the normalized throughput changes
            2.  Given same amount of sleeping time (Sleep for 10us each iterations)
                  Tune the level of parallization to see how normalized throughput changes
                  (10 OpenMPs <-> 10 OpenMPs) vs (20 OpenMPs <-> 20 OpenMPs)
                  (10 MPIs <-> 10 MPIs) vs (20 MPIs <-> 20 MPIs)
            3.  Show hybrid model is inferior the native MPI model
                  (10 MPIs <-> 10 MPIs) vs (10 OpenMPs <-> 10 OpenMPs)

      4. Usage

          To run twoSidedSleepSend_Multi_MPIs:
              srun -N 1 -n <# of MPIs> ./twoSidedSleepSend_Multi_MPIs -B <SLEEP_BASE in us> -F <SLEEP_FLUCTUATION in us>

          To run twoSidedSleepSend_Multi_OpenMP:
              export MPICH_MAX_THREAD_SAFETY=MPI_THREAD_MULTIPLE
              export OMP_NUM_THREADS=64
              srun -N 1 -n 2 ./twoSidedSleepSend_Multi_OpenMP -B <SLEEP_BASE in us> -F <SLEEP_FLUCTUATION in us> -T <# of OpenMP threads per MPI>

      5. Observations (all the performance is evaluated based on the normalized throughput)
          1. Pure MPI version is indifferent to the scaling up parallization (expected)
            and is indifferent to the length of sleeping time (expected)
          2. Hybrid does perform much worse compared to pure MPI especially
            when the sleeping time per thread is short (expected)
            as it implies multiple threads are contending for MPI internal handle.
          3. Hybrid model performs worse when the level of parallization goes up (expected) due to the same reason.
          4. Hybrid model however does perform much better when the length of sleeping time grows larger (expected)
            as fewer threads are contending for the MPI internal handle. Some level of parallization is achieved.
              Especially when the length of sleeping time is around 100 - 150us per iteration,
            the throughput is comparable to its MPI counterpart.
